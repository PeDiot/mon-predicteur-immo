---
title: " Data Exploration : Demandes de valeurs foncières"
author: "Pierre-Emmanuel Diot"
date: "Last updated: `r Sys.time()`"
output: 
  bookdown::html_document2: 
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: false
editor_options: 
  chunk_output_type: console
params: 
  year: 2021
---

<style type="text/css">
.twoC {width: 100%}
.clearer {clear: both}
.twoC .table {max-width: 50%; float: right}
.twoC img {max-width: 50%; float: left}
</style>

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      comment = FALSE, 
                      warning = FALSE, 
                      fig.align = "center", 
                      fig.height = 3.5, 
                      fig.width = 4)

```

```{r eval=FALSE}
root <- "C:/Users/pemma/OneDrive - GENES/Ensae/Business Data Challenge/business-data-challenge/code/exploration"
setwd(root)
```

```{r dir}
DIR <- getwd()

DATA_DIR <- "../../data/"
BACKUP_DIR <- "../../backup/"

ZIP_DIR <- paste0(DATA_DIR, "dvf.zip") 
ZIP_CODE_DIR <- paste0(DATA_DIR, "french-zip-code/")
DENSITY_DIR <- paste0(DATA_DIR, "densite/")
```


```{r packages}
library(rmarkdown)

library(tidyverse)
library(data.table)
library(DT)

library(naniar)
library(questionr)

library(stargazer)

library(kableExtra)

library(shiny)
library(plotly)
library(leaflet)
library(RColorBrewer)
library(scales)
```

```{r}
theme_set(theme_minimal(base_size = 9))
```


```{r functions}
source("functions.R", encoding = "UTF-8")
```

```{r eval=FALSE}
files <- unzip(ZIP_DIR, list = T) %>% 
  mutate(Year = Name %>% parse_number())
```


## Data

L'étude porte sur les demandes de valeurs foncières pour l'année `r params$year`. 

```{r eval=FALSE}
file_name <- paste0("dvf/", params$year, ".csv")
file_path <- unzip(ZIP_DIR, file_name)

dat <- read_csv(file_path)
```


### Filtre 

L'objectif est de construire un prédicteur du prix de vente des maisons & appartements. Nous décidons d'appliquer un filtre sur la base de données consistant à sélectionner les observations telles que :

- `nature_mutation` = "Vente", 
- `type_local` = "Maison" ou `type_local` = "Appartement".

Avant d'appliquer le filtre, nous pouvons recoder la variable `type_local` de telle sorte à obtenir 4 catégories identifiées à partir de la variable `code_type_local`.

```{r }
# tab <- table(dat$type_local, dat$code_type_local) %>% 
#   prop.table() 
# colnames(tab) <- paste("code", colnames(tab), sep = "=")

file_path <- paste0(BACKUP_DIR, "exploration/tab_type_local.RData")
# save(tab, file = file_path)

load(file_path)

tab %>% 
  mykable(title = "Tableau de proportions entre code_type_local et type_local", 
          digits = 3)
```

```{r eval=FALSE}
dat <- dat %>% 
  
  mutate(type_local = if_else(
    
    code_type_local == 1, 
    
    "Maison", 
    
    if_else(code_type_local == 2, 
            
            "Appartement", 
            
            if_else(code_type_local == 3, 
                    
                    "Dépendance", 
                    "Local industriel. commercial ou assimilé"))
  ))
```

```{r cleaneddata}
# dat2 <- dat %>%
#   filter(nature_mutation == "Vente") %>%
#   filter(type_local %in% c("Appartement", "Maison")) 

file_path <- paste0(DATA_DIR, "dvf_rdata/df_filtered_", params$year, ".RData")
# save(dat2, file = file_path)

load(file_path)
```

```{r}
n2 <- dat2 %>% nrow() %>% format(big.mark = " ", scientific = F)
```

Après l'application du filtre, on obtient un jeu de données à `r n2` transactions.

```{r}
random_idxs <- rdunif(n = 10, b = nrow(dat2)) %>% as.character()

dat2 %>% 
  filter(rownames(.) %in% random_idxs) %>% 
  mykable(title = "Aperçu de la base de données après application du filtre") %>% 
  scroll_box(width = "100%", box_css = "border: 0px;")
```

### Encodage de la variable `surface_terrain`

La table suivante indique que la variable `surface_terrain` contient 75% de données manquantes pour les biens de type "Appartement". On décide alors de mettre à 0 les observations telles que `type_local` = "Appartement" et `surface_terrain` = `NA`.  

```{r}
tab <- sapply(
  X = c("Maison", "Appartement"), 
  
  FUN = function(type) {
    
    tmp <- dat2 %>% filter(type_local == type)
    
    tab <- tmp %>% 
      select(surface_terrain) %>% 
      freq.na() %>%
      t() %>%
      as.data.frame() %>%
      mutate(N = nrow(tmp)) %>% 
      mutate_at(vars(missing, N), 
                format, 
                scientific = F, 
                big.mark = " ") %>%
      relocate(N, .before = missing)
    
    return (tab)
    
  }, 
  
  simplify = T, 
  USE.NAMES = T
  
)


tab %>% 
  mykable(title = "Données manquantes pour la surface du terrain selon le type de bien")
```

```{r}
dat2[dat2$type_local == "Appartement", "surface_terrain"] <- dat2 %>%
  filter(type_local == "Appartement") %>%
  mutate(surface_terrain = replace_na(surface_terrain, 0)) %>%
  pull(surface_terrain)
```


### Ajout des régions

```{r regions}
dpts <- read_csv(paste0(ZIP_CODE_DIR, "departments.csv")) %>% 
  rename(code_departement = code, 
         nom_departement = name, 
         code_region = region_code) %>% 
  select(-c(id, slug))

regions <- read_csv(paste0(ZIP_CODE_DIR, "regions.csv")) %>%
  rename(code_region = code, 
         nom_region = name) %>% 
  select(-c(id, slug)) 

regions_dpts <- inner_join(x = dpts, y = regions, by = "code_region")

dat2 <- left_join(dat2, regions_dpts, by = "code_departement")
```

### Ajout des variables de densité / population

On récupère une grille communale de densité depuis le [site](https://www.insee.fr/fr/information/6439600) de l'Insee. Cette grille comprend le code de la commune, son nombre d'habitants (2019) et une variable `degre_densite` à 7 niveaux : 

1. "Rural à habitat dispersé"
2. "Centres urbains intermédiaires"
3. "Bourgs ruraux"       
4. "Ceintures urbaines"            
5. "Rural à habitat très dispersé"
6. "Petites villes"                
7. "Grands centres urbains"

On décide d'y ajouter le nombre d'habitants pour les arrondissements de Paris, Marseille et Lyon. Pour ces 3 villes, la variable `degre_densite` vaut respectivement "Paris", "Marseille" et "Lyon". 


```{r }
# census data for all french towns
densite <- readxl::read_xlsx(
  path = paste0(DENSITY_DIR, "grille_densite_7_niveaux_detaille_2022.xlsx")) %>% 
  dplyr::rename(
    code_commune = `Code commune`, 
    degre_densite = `Libellé degré densité`, 
    pop = `Population \r\nmunicipale \r\n2019`
  ) %>%
  select(c(code_commune, degre_densite, pop))

# add paris neighborhoods 
# source: https://94.citoyens.com/2019/population-2019-a-paris-par-arrondissement,07-01-2019.html
densite_paris <- read_csv(file = paste0(DENSITY_DIR, "population_paris.csv")) %>%
  rename(nom_commune = `Nom de la commune`) %>%
  filter(nom_commune != "Total") %>%
  select(1:2)
colnames(densite_paris)[2] <- "pop"

paris_neighborhoods_code <- dat2 %>% 
  filter(nom_commune %in% densite_paris$nom_commune) %>% 
  select(c(nom_commune, code_commune)) %>%
  unique()

densite_paris <- inner_join(densite_paris, 
                            paris_neighborhoods_code, 
                            by = "nom_commune") %>% 
  select(-nom_commune) %>%
  mutate(degre_densite = rep("Paris", nrow(.))) %>%
  relocate(code_commune, .before = "pop") %>% 
  relocate(degre_densite, .before = "pop") 

# add marseille neighborhoods (wiki)
densite_marseille <- read_csv(
  file = paste0(DENSITY_DIR, "population_marseille.csv")) %>% 
  mutate(degre_densite = rep("Marseille", nrow(.))) %>%
  select(c(code_commune, degre_densite, pop))

# add lyon neighborhoods (wiki)
densite_lyon <- read_csv(
  file = paste0(DENSITY_DIR, "population_lyon.csv")) %>% 
  mutate(degre_densite = rep("Lyon", nrow(.))) %>%
  select(c(code_commune, degre_densite, pop))

densite <- rbind(densite, 
                 densite_paris, 
                 densite_marseille, 
                 densite_lyon)

dat2 <- left_join(dat2, densite, by = "code_commune")
```


### Visualisation géographique

<center>

```{r}
samp <- dat2 %>%
  filter(rownames(.) %in% sample(x = 1:nrow(dat2), size = 2000)) 

leaflet() %>% 
     addTiles() %>% 
     setView(lng = 2.80, lat = 46.80, zoom = 5.1) %>% 
     addCircleMarkers(
       lng = samp$longitude,
       lat = samp$latitude, 
       radius = 0)
```

<center/>


### Données manquantes {#missing}

```{r}
# missing <- dat2 %>% freq.na() %>% as.data.frame() 

file_name <- paste0(BACKUP_DIR, "exploration/missing_dvf_", params$year, ".RData")
# save(missing, file = file_name)

load(file = file_name)
```

```{r}
vars_with_nans <- missing %>% 
  dplyr::filter(missing > 0) %>% 
  rownames()

p <- dat2 %>%
  select(all_of(vars_with_nans)) %>% 
  gg_miss_var(show_pct = T) +
  labs(x = "") 
```


:::: {style="display: flex;"}

::: {}

```{r missingtab}
missing %>% 
  dplyr::filter(missing > 0) %>%
  datatable()
```

:::

::: {}

```{r fig.height=5}
p 
```
:::

::::

### Description des variables 

<center>

![](../../imgs/dvf_vars1.png)

</center>

<center>

![](../../imgs/dvf_vars2.png)
</center>

<center>

![](../../imgs/dvf_vars3.png)

</center>

<center>

![](../../imgs/dvf_vars4.png)

</center>

### Types de variables

**Variables identifiantes** 

```{r}
col_id <- c(
  "id_mutation",
  "date_mutation",
  "adresse_numero",
  "adresse_suffixe",
  "adresse_code_voie",
  "adresse_nom_voie", 
  "code_postal", 
  "code_commune",
  "nom_commune", 
  "ancien_code_commune",
  "ancien_nom_commune", 
  "code_departement",
  "code_region", 
  "id_parcelle",
  "ancien_id_parcelle", 
  "numero_volume",
  "numero_disposition", 
  "latitude", 
  "longitude", 
  "lot1_numero", 
  "lot2_numero", 
  "lot3_numero", 
  "lot4_numero", 
  "lot5_numero"
)

dat_id <- dat2 %>% dplyr::select(all_of(col_id))
```

```{r}
make_structure_table(dat_id, missing) %>% datatable()
```

**Variables catégorielles**

```{r}
col_cat <- c(
  "type_local",
  "nombre_pieces_principales", 
  "nature_culture", 
  "nature_culture_speciale", 
  "nom_departement", 
  "nom_region", 
  "degre_densite"
)

dat2 <- dat2 %>%
  mutate(across(all_of(col_cat), as.factor)) 

dat_cat <- dat2 %>%
  select(all_of(col_cat)) 
```


```{r}
make_structure_table(dat_cat, missing) %>% datatable()
```

**Variables quantitatives**

```{r}
col_quanti <- c(
  "valeur_fonciere", 
  "surface_reelle_bati", 
  "surface_terrain", 
  "lot1_surface_carrez", 
  "lot2_surface_carrez", 
  "lot3_surface_carrez", 
  "lot4_surface_carrez", 
  "lot5_surface_carrez", 
  "pop")

dat_quanti <- dat2 %>% dplyr::select(all_of(col_quanti))
```

```{r}
make_structure_table(dat_quanti, missing) %>% datatable()
```

## Statistiques descriptives

### Variables catégorielles

#### Répartition par catégorie

```{r}
n_vars <- ncol(dat_cat)
colors <- brewer.pal(n_vars, "Set3")

plot_list <- list()
for (i in 1:n_vars){
  var_name <- colnames(dat_cat)[i]
  n_levels <- nlevels(dat_cat[[var_name]])
  
  if (missing[var_name, "%"] < 100) 
    plot_list[[var_name]]<- make_pct_countplot(df = dat_cat, var_name = var_name)
  
}
```

:::: {style="display: flex;"}

::: {}
```{r }
plot_list$type_local
```
:::

::: {}

```{r fig.width=6, fig.height=4}
plot_list$nombre_pieces_principales
```
:::

::::


:::: {style="display: flex;"}

::: {}
```{r }
plot_list$nom_region
```
:::

::: {}

```{r }
plot_list$degre_densite
```
:::

::::

<center>

```{r fig.width=6, fig.height=4}
plot_list$nature_culture_speciale
```

</center>

<center>

```{r fig.width=6, fig.height=4}
plot_list$nature_culture
```

</center>


```{r eval=FALSE}
table(dat2$nature_culture, dat2$code_nature_culture)
```

#### Nombre de pièces principales

```{r}
n_levels <- dat2$nombre_pieces_principales %>% nlevels()
```

Selon l’article R.111-1-1 du Code de la Construction, une pièce principale d’un logement est une pièce de vie d’une surface de plus de 9m² et 1,8m de hauteur sous plafond. Ne sont pas des pièces principales les pièces de service comme la cuisine, la salle de bains ou les WC. Au delà d’une surface de 30 m² les assurances comptent deux pièces principale ([source](https://eldorado-immobilier.com/combien-de-pieces-principales/)). 

La variable `nombre_pieces_principales` comporte `r n_levels` catégories dont certaines sont peu représentées. On décide de procéder au découpage suivant afin de simplifier cette variable : 

- 0 pièce
- 1 pièce
- 2 pièces
- 3 pièces
- 4 pièces 
- 5 à 7 pièces
- 8 et +

```{r}
recode_number_of_rooms <- function(x) {
  
  x_int <- as.integer(x)
  x <- as.character(x)
  
  if_else(
    
      x_int <= 5, 
      x, 
      
      if_else(
        x_int < 8, 
        "5-7", 
        "8+"
      )
      
    )
}

nb_rooms <- dat2 %>% 
  pull(nombre_pieces_principales) %>%
  recode_number_of_rooms()

levels <- c(
  seq(0, 4) %>% as.character(), 
  "5-7", 
  "8+"
)

nb_rooms_fct <- factor(x = nb_rooms, levels = levels)

dat2 <- dat2 %>%
  mutate(nombre_pieces_principales2 =  nb_rooms_fct)

dat_cat <- dat_cat %>%
  mutate(nombre_pieces_principales2 = dat2$nombre_pieces_principales2) %>%
  select(-nombre_pieces_principales)

col_cat <- colnames(dat_cat)
```


:::: {style="display: flex;"}

::: {}
```{r }
freq(dat2$nombre_pieces_principales) %>% 
  arrange(desc(n)) %>% 
  rownames_to_column(var = "Nombre de pièces principales") %>%
  datatable()
```
:::

::: {}

```{r fig.width=6, fig.height=4}
plot_list$nombre_pieces_principales
```
:::

::::

<center>

```{r}
make_pct_countplot(dat_cat, var_name = "nombre_pieces_principales2")
```

</center>


#### Les lots 

On remarque que les variables liées aux lots contiennent beaucoup de données manquantes (cf: section [Données manquantes](#missing)). Un lot de copropriété est constitué d’une partie
privative (appartement, cave, etc.) et d’une quote-part de partie commune (tantièmes). Seuls les 5 premiers lots sont mentionnés. Si le nombre de lots est supérieur à 5, ils ne sont pas restitués. Il existe donc des biens qui n'ont pas de lots. 

On ajoute ainsi une variable qui compte le nombre de lots qu'un bien possède. On nomme cette variable `nombre_lots`. 

```{r}
lot_vars <- lapply(X = seq(1, 5), 
                   FUN = function(i) paste0("lot", i, "_numero")) %>% unlist()

lots <- dat2 %>%
  select(all_of(lot_vars)) %>%
  mutate_at(vars(everything()), ~replace_na(., 0)) %>%
  as.matrix()

dat2 <- dat2 %>% 
  mutate(nombre_lots = factor(
    x = rowSums(lots != 0), 
    levels = seq(0, length(lot_vars))) )

dat_cat <- dat_cat %>% 
  mutate(nombre_lots = dat2$nombre_lots)

col_cat <- colnames(dat_cat)
```


On décide aussi de créer la variable dummy `lot` qui vaut 1 si `nombre_lots` > 0, 0 sinon.

```{r}
dat2 <- dat2 %>%
  mutate(lot = factor(
    x = if_else(nombre_lots == "1", 1, 0), 
    levels = c(0, 1)
  ))

dat_cat <- dat_cat %>% 
  mutate(lot = dat2$lot)

col_cat <- colnames(dat_cat)
```

:::: {style="display: flex;"}

::: {}
```{r }
make_pct_countplot(dat_cat, var_name = "nombre_lots")
```
:::

::: {}

```{r}
make_pct_countplot(dat_cat, var_name = "lot")
```
:::

::::


### Variables quantitatives

#### Surface carrez des lots 

On décide aussi de sommer les variables `lot1_surface_carrez`, ..., `lot5_surface_carrez` afin de créer une variable globale `surface_carrez_lots`. Cette variable est nulle pour les biens n'ayant pas de lots. 


```{r}
dat2 <- dat2 %>%
  mutate_at(vars(ends_with("surface_carrez")), ~replace_na(., 0)) %>% 
  mutate(surface_carrez_lots = lot1_surface_carrez + 
           lot2_surface_carrez + 
           lot3_surface_carrez + 
           lot4_surface_carrez + 
           lot5_surface_carrez)
  

dat_quanti <- dat_quanti %>%
  mutate(
    surface_carrez_lots = dat2$surface_carrez_lots) %>% 
  select(-ends_with("_surface_carrez"))

col_quanti <- colnames(dat_quanti)
```


#### Résumé statistique

```{r}
stats_quanti <- sapply(
  
  X = col_quanti, 
  FUN = function(var_name) {
   x <- dat_quanti %>% pull(var_name)
   stat_table(x, var_name)
  }, 
  
  USE.NAMES = T, 
  simplify = F
) 

stats_quanti_tab <- stats_quanti %>%
  bind_rows() %>% 
  filter(rownames(.) != "valeur_fonciere") %>% 
  mutate_if(is.numeric, funs(format(., big.mark = " ", scientific = F, digits = 2)))
```

Dans le tableau suivant, `N` représente le nombre de données disponibles pour la variable considérée dans la base de données `r params$year`.


```{r statsquantitab}
stats_quanti_tab %>% 
  mykable(title = "Résumé statistique des variables quantitatives")
```

```{r}
most_populated <- dat2 %>%
  filter(pop == max(pop, na.rm = T)) %>%
  pull(nom_commune) %>%
  unique()
```


On remarque que l'étendue de la distribution des différentes variables quantitatives est importante. Notons aussi que la valeur maximale de population est 493 465 et correspond à la ville de `r most_populated` car on considère les villes de Paris, Marseille et Lyon par arrondissement. 

Pour représenter les histogrammes des variables `surface_relle_bati`, `surface_terrain` et `pop` on décide de sélectionner les valeurs **strictement positives** et inférieures ou égales au 3ème quantile de chaque série. Notons que la variables `surface_terrain` comporte un grand nombre de valeurs nulles puisque l'on a affecté la valeur 0 à l'ensemble des appartements pour lequels la surface du terrain n'était pas renseignée. 

Pour la variable `surface_carrez_lots`, on sélectionne uniquement les biens pour lesquels cette variable est non nulle et on applique le même filtre que pour les deux autres variables.

#### Distribution 


```{r}
n_vars <- ncol(dat_quanti)
colors <- brewer.pal(n_vars, "Set3")

plot_list <- list()
for (i in 1:n_vars) {
  
  var_name <- col_quanti[i]
  
  thresold <- stats_quanti[[var_name]][var_name, "Q75%"] 
  if (thresold == 0) {
    thresold <- stats_quanti[[var_name]][var_name, "Max"] 
  }
  
  df_plot <- dat_quanti %>%
    filter(!!sym(var_name) > 0 & !!sym(var_name) <= thresold)
  
  
  color <- colors[i]
  plot_list[[var_name]] <- make_histogram(df_plot, var_name = var_name, color = color)

}
```

:::: {style="display: flex;"}

::: {}
```{r }
plot_list$surface_reelle_bati
```
:::

::: {}

```{r }
plot_list$surface_terrain
```
:::

::::

<center>

```{r }
plot_list$surface_carrez_lots
```

</center>

<center>

```{r }
plot_list$pop
```

</center>

### Variable cible : `valeur_fonciere`


```{r }
stats_quanti$valeur_fonciere %>% 
  mutate_if(is.numeric, funs(format(., big.mark = " ", scientific = F, digits = 2))) %>% 
  mykable(title = "Résumé statistique de la variable cible")
```

<center>

```{r}
plot_list$valeur_fonciere
```

</center>


## Analyse approfondie

### Valeurs abberantes via le filtre d'Hampel

La table \@ref(tab:statsquantitab) indique des disparités importantes au sein de chaque variable quantitative. L'objectif est d'identifier les valeurs aberrantes à l'aide d'une méthode statistique. On décide d'utiliser le filtre d'Hampel qui classe comme outliers les valeurs situés en dehors de l'intervalle $I$ défini comme suit :

$$
I = [\tilde{X} - 3\times \text{MAD} ; \tilde{X} + 3\times \text{MAD}] \quad \text{avec MAD}= \text{median}(|X_i - \tilde{X}|)
$$
où $\tilde{X}$ est la médiane de la série $X$ et $\text{MAD}$ l'écart absolu médian.

```{r}
outliers <- sapply(
  
  X = col_quanti, 
  
  FUN = function(var_name) {
    
    # consider only positive values for surface_carrez_lots
    if (var_name == "surface_carrez_lots") {
      tmp <- dat2 %>%
        filter(surface_carrez_lots > 0)
    }
    else { tmp <- dat2 }
    
    tmp %>% 
      pull(var_name) %>% hampel_filter()
    
  }
    
)

n <- nrow(dat2) 

outliers_freqs <- sapply(
  X = outliers, 
  FUN = function(x) length(x) / n
 ) %>% 
  bind_rows() %>%
  t() %>% 
  as.data.frame() %>% 
  rename(`%` = V1)
```

29% de points aberrants sont identifiés par le filtre d'Hampel au regard de la variable `pop`. Il semble intéressant de visualiser la position géographique de ces points. On remarque que la majorité de ces biens sont situés dans les villes de taille importante.

<center>

```{r}
outliers_pop <- dat2 %>%
  filter(rownames(.) %in% outliers$pop) %>%
  filter(duplicated(nom_commune) == FALSE)

outliers_pop %>%
  leaflet() %>% 
     addTiles() %>% 
     setView(lng = 2.80, lat = 46.80, zoom = 5.1) %>% 
     addCircleMarkers(
       lng = ~longitude,
       lat = ~latitude, 
       radius = 0, 
       color = "red")
```

</center>

L'identification des valeurs aberrantes pour la variable `surface_carrez_lots` porte sur les biens possédant des lots. On considère ici que les biens pour lesquels `surface_carrez_lots` = 0 ne sont pas des point aberrants.  

```{r}
outliers_freqs %>% 
  arrange(desc(`%`)) %>% 
  mutate_if(is.numeric, funs(format(100 * ., big.mark = " ", scientific = F, digits = 2))) %>% 
  mykable(title = "Proportion d'outliers par variable quantitative")
```


```{r}
to_remove <- outliers %>% 
  unlist() %>% 
  unique() %>% 
  as.character()

dat_no_outliers <- dat2 %>% 
  filter(!(rownames(.) %in% to_remove)) %>%
  filter(as.numeric(nombre_pieces_principales) <= 12) 

n <- dat2 %>% nrow() %>% format(big.mark = " ", scientific = F)
n_bis <- dat_no_outliers %>% nrow() %>% format(big.mark = " ", scientific = F)
```

On décide de supprimer les valeurs aberrantes identifiées par le filtre d'Hampel : 

- `r n` observations dans la base de données initiale
- `r n_bis` observations après suppressions des outliers


```{r}
n_samples <- format(10000, big.mark = " ", scientific = F)
```

Afin de pouvoir construire des visualisations assez rapidement, on décide de travailler sur un `r n_samples`-échantillon de la base de données sans outliers. Concernant la variable `surface_carrez_lots`, on construits l'échantillon à partir des biens possédants des lots (`surface_carrez_lots` > 0).

### Variables catégorielles & `valeur_fonciere`

#### Distribution par catégorie

```{r}
cat_vars <- setdiff(x = col_cat, 
                    y = c("code_nature_culture", 
                          "code_nature_culture_speciale", 
                          "nature_mutation"))

target <- "valeur_fonciere"
```


```{r}
target <- "valeur_fonciere"
idxs <- outliers$valeur_fonciere %>% as.character()

plot_list <- list()

for (var in cat_vars) {
  
  tmp <- dat2 %>%
    select(c(
      all_of(target), 
      all_of(var)
    )) %>%
    filter(!(rownames(.) %in% idxs)) %>%
    draw_samples()
  
  p <- tmp %>% boxplot_per_category(target = target, by = var) 
  plot_list[[var]] <- p 
  
}
```

:::: {style="display: flex;"}

::: {}
```{r }
plot_list$degre_densite
```
:::

::: {}

```{r fig.width=5}
plot_list$nom_region
```
:::

::::

:::: {style="display: flex;"}

::: {}
```{r }
plot_list$nombre_pieces_principales2
```
:::

::: {}

```{r }
plot_list$type_local
```
:::

::::

:::: {style="display: flex;"}

::: {}
```{r }
plot_list$nombre_lots
```
:::

::: {}

```{r }
plot_list$lot
```
:::

::::

<center>

```{r fig.width=7, fig.height=4}
plot_list$nature_culture
```

</center>

<center>

```{r fig.width=8, fig.height=5}
plot_list$nature_culture_speciale
```

</center>


#### Tests statistiques

On utilise à nouveau le jeu de données diminué des outliers identifiés avec le filtre d'Hampel afin d'effectuer les tests statistiques. L'objectif est de déterminer s'il y a des différences significatives de valeurs foncières entre les différents catégories des variables `type_local` et `nombre_pieces_principales2`.

**Valeur foncière entre maisons et appartement** 

```{r}
vf_houses <- dat_no_outliers %>% 
  filter(type_local == "Maison") %>%
  pull(valeur_fonciere)

vf_flats <- dat_no_outliers %>% 
  filter(type_local == "Appartement") %>%
  pull(valeur_fonciere)

t_results <- t.test(
  x = vf_houses, 
  y = vf_flats, 
  alternative = "greater")
```

```{r}
tab <- c(t_results$estimate, t_results$p.value) %>% 
  format(scientific = F, big.mark = " ") %>% 
  as.data.frame() %>% 
  t()

colnames(tab) <- c("Maison", "Appartement", "p value")
rownames(tab) <- ""

tab %>%
  mykable(title = "Résultats du test de Student entre valeurs foncières des maisons et des appartements")
```


**Valeurs foncières et variables à 2+ catégories**

Les variables `noms_regions`, `degre_densite`, `nombre_pieces_principales2` et `nombre_lots` contenant plus de 2 catégories, on effectue un test ANOVA. L'ANOVA permet de voir si une variable numérique a des valeurs différentes en fonction de plusieurs groupes. C'est une généralisation du test de Student permettant de comparer plus de deux groupes.

```{r}
target <- "valeur_fonciere"

aov_fits <- sapply(
  
  X = c("nom_region", "degre_densite", "nombre_pieces_principales2", "nombre_lots"), 
  
  FUN = function(var) {
    fm <- paste(target, var, sep = "~") %>% as.formula()
    fit <- aov(fm, data = dat_no_outliers) 
    
    res <- summary(fit) %>%
      unclass() %>%
      as.data.frame() 
    
    return (res[var, ])
    
  }, 
  
  USE.NAMES = T, 
  simplify = F
) %>% bind_rows()
  
```

```{r}
aov_fits %>%
  mykable(title = "Résumé des tests ANOVA")
```

Il y a bien des différences siginificatives de valeur foncière entre biens ayant un nombre de pièces principales (resp. lots) différent, les biens venant de régions différentes et les biens situés dans des aires géographiques à degré de densité différent. Une régression linéaire permettrait d'estimer l'effet marginal du nombre de pièces (resp. lots) sur la valeur foncière et les différences de valeur foncière entre les régions. 

### Variables quantitatives & `valeur_fonciere`

#### Nuages de points

```{r}
vars <- setdiff(col_quanti, "valeur_fonciere")
n_vars <- length(vars)
target <- "valeur_fonciere"

colors <- brewer.pal(n_vars, "Set3")

n_samples <- 10000

plot_list <- list()
for (i in 1:n_vars) {
  
  x_var <- vars[i]
  c <- colors[i]
  
  if (x_var %in% c("surface_carrez_lots", "surface_terrain")) {
    tmp <- dat_no_outliers %>% 
      filter(!!sym(x_var) > 0)
  }
  else { tmp <- dat_no_outliers }
  
  df_subset <- tmp %>%
      dplyr::select(c(all_of(x_var), all_of(target))) %>% 
    draw_samples(n_samples = n_samples)
  
  plot_list[[x_var]] <- make_scatter_plot(df = df_subset, 
                                          x_var = x_var, 
                                          y_var = target, 
                                          color = c)

}

```

<center>

```{r }
plot_list$pop
```

</center>

:::: {style="display: flex;"}

::: {}
```{r }
plot_list$surface_reelle_bati
```
:::

::: {}

```{r }
plot_list$surface_terrain
```
:::

::::

<center>

```{r }
plot_list$surface_carrez_lots
```

</center>

#### Matrice des corrélations 

Afin d'avoir une idée plus précise des corrélations entre les variables quantitatives de la base de données et `valeur_fonciere`, on décide de calculer le coefficient de corrélation linéaire de Pearson sur le jeu de données sans outliers. 

```{r}
cor_no_outliers <- dat_no_outliers %>%
  select(all_of(col_quanti)) %>% 
  cor(use = "complete.obs")

cor_plot <- ggcorrplot::ggcorrplot(
  cor_no_outliers, 
  hc.order = TRUE, 
  type = "lower",
  outline.col = "white",
  ggtheme = theme_minimal(),
  show.legend = F, 
  colors = c("darkred", "white", "darkgreen")) 
```

<center>

```{r fig.width=5, fig.height=4}
ggplotly(cor_plot)
```

</center>

### `valeur_foncière` selon la localisation 

Il semble enfin pertinent de représenter la répartition géographique des biens selon leur valeur foncière. On travaille ici sur le jeu de données diminué des outliers. Afin d'éviter des temps d'affichage trop long, on travaille sur un 10 000-échantillon. 

```{r}
samp <- dat_no_outliers %>%
  filter(rownames(.) %in% sample(x = 1:nrow(dat_no_outliers), 
                                 size = 10000))

pal <- colorNumeric(
  palette = "Blues",
  domain = samp$valeur_fonciere)

map_no_outlier <- samp %>%
  leaflet() %>% 
     addTiles() %>% 
     setView(lng = 2.80, lat = 46.80, zoom = 5.1) %>% 
     addCircleMarkers(
       lng = ~longitude,
       lat = ~latitude, 
       radius = 0, 
       color = ~pal(valeur_fonciere))
```

<center>

```{r}
map_no_outlier
```

</center>


## Régression linéaire

### OLS

#### Premier modèle

On décide de mettre la variable `valeur_foncière` au logarithme pour les modèles de régression afin de réduire les effest d'échelles. On nomme cette variable `l_valeur_fonciere`. 

La régression comporte des variables catégorielles dont les catégories de référence sont les suivantes :

| Variable  | Référence  |
|---|---|
| `nombre_pieces_principales` | 0  |
| `nom_region`  | "Île-de-France" |
| `degre_densite` | "Paris" | 
| `lot` | 0 | 

```{r}
x_vars <- c(
  "type_local", 
  "nombre_pieces_principales", 
  "nom_region", 
  "degre_densite", 
  "surface_terrain",
  "surface_reelle_bati", 
  "lot"
)


dat_mod <- dat2 %>%
  select(-nombre_pieces_principales) %>% 
  rename(nombre_pieces_principales = nombre_pieces_principales2) %>% 
  
  select(all_of(x_vars), valeur_fonciere) %>%
  
  mutate(l_valeur_fonciere = log(valeur_fonciere), 
         nom_region = relevel(nom_region, ref = "Île-de-France"), 
         degre_densite = relevel(degre_densite, ref = "Paris"))
```

<center>

```{r}
make_histogram(dat_mod, "l_valeur_fonciere")
```

</center>


```{r}
y_var <- "l_valeur_fonciere"

fm <- paste(y_var, 
            paste(x_vars, collapse = "+"), 
            sep = "~")

lm1 <- lm(formula = fm, data = dat_mod)
```

```{r eval=FALSE}
stargazer(lm1, type = "text")
```

<center>

```{r results='asis'}
stargazer(lm1, type = "html", title = "Premier modèle") 
```

</center>

L'ensemble des coefficients sont significatifs. Cependant, la part de variabilité du logarithme des valeurs foncières expliquée par le modèle est seulement de 18.2%. 

#### Outliers & Distance de Cook

La distance de Cook est une mesure calculée par rapport à un modèle de régression donné et n'est donc influencée que par les variables $X$ incluses dans le modèle. La distance de Cook pour chaque observation $i$ mesure le changement de $\hat{y}$ pour toutes les observations avec et sans la présence de l'observation $i$. Nous savons donc dans quelle mesure l'observation $i$ a eu un impact sur les valeurs ajustées. Mathématiquement, la distance de Cook $D_i$ pour l'observation $i$ est calculée comme suit :

$$
D{_i}=\frac{\sum_{j=1}^{n}\left( \hat{y}_{j} - \hat{y}_{j \left(i \right)} \right)^{2}}{p \times \text{MSE}}
$$
avec :

- $\hat{y}_{j}$ la valeur de la $j$-ème valeur ajustée lorsque toutes les observations sont incluses. 
- $\hat{y}_{j \left(i \right)}$ la $j$-ème valeur ajustée lorsque l'observation $i$ n'est pas incluse dans l'estimation. 
- $\text{MSE}$ l'erreur quadratique moyenne tq $\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$
- $p$ le nombre de coefficients dans le modèle de régression. 

Le point $i$ est considéré comme un outlier si $D_i > 4 \times \bar{D}$, avec $\bar{D} = \frac{1}{n} \sum_{i=1}^n D_i$. 

```{r}
cooksd <- cooks.distance(lm1)

cooksd_df <- data.frame(obs = 1:length(cooksd), distance = cooksd)
frontier <- 4 * mean(cooksd, na.rm = TRUE)

cooksd_df <- cooksd_df %>%
  mutate(influential = if_else(distance > frontier, "yes", "no")) 
```

```{r}
tab <- cooksd_df %>% 
  pull(influential) %>%
  table() %>% 
  prop.table() %>%
  as.data.frame() %>%
  mutate(N = format(Freq * nrow(cooksd_df), 
                    big.mark = " ", scientific = F)) %>% 
  mutate(Freq = 100 * Freq) 

colnames(tab) <- c("Influential", "%", "N") 

tab %>% 
  mykable(title = "Proportions d'observations influentes selon la distance de Cook")
```

```{r}
idxs_outliers <- cooksd_df %>% filter(influential == "yes") %>% rownames()
n_outliers <- length(idxs_outliers)

idxs <- cooksd_df %>% filter(influential == "no") %>% rownames()
idxs_smp <- sample(x = idxs, size = n_outliers)

smp_outliers <- dat_mod %>%
  filter(rownames(.) %in% c(idxs_smp, idxs_outliers)) %>%
  mutate(
    fitted = c(
      lm1$fitted.values[idxs_outliers], 
      lm1$fitted.values[idxs_smp]
    ), 
    outlier = c(
      rep("yes", n_outliers), 
      rep("no", n_outliers)
    )
  ) %>% mutate(outlier = factor(outlier, levels = c("yes", "no")))
```

Une première visualisation consiste à représenter valeurs observées vs valeurs estimées pour `l_valeur_fonciere` et de séparer les points aberrants des points non abberants. En comparant les résultats à la droite d'équation $y=x$ on comprend que la qualité d'ajustement du premier modèle est faible. 

<center>

```{r fig.width=5}
title <- "Logarithme des valeurs foncières"
smp_outliers %>%
  rename(actual = l_valeur_fonciere) %>%
  regression_plot(title=title, by = "outlier")
```

</center>

#### Analyse des outliers


```{r}
plot_list <- sapply(
  
  X = c("type_local",
        "nombre_pieces_principales",
        "lot",
        "nom_region", 
        "degre_densite"), 
  
  FUN = barplot_per_category, 
  df = smp_outliers, 
  cat_var2 = "outlier", 
  
  simplify = F, 
  USE.NAMES = T
  
)
```

On construit sous-échantillon de taille `r nrow(smp_outliers)` à partir du jeu de données dans lequel la proportion d'outliers est identique à celle des observations "normales" au sens du modèle de régression. 

On commence par représenter la répartition des variables catégorielles du modèle pour les points aberrants et les points non aberrants.


:::: {style="display: flex;"}

::: {}
```{r }
plot_list$nombre_pieces_principales
```
:::

::: {}

```{r }
plot_list$type_local
```
:::

::::

<center>

```{r }
plot_list$type_local
```

</center>

<center>

```{r fig.width=5, fig.height=4.5}
plot_list$nom_region
```

</center>

<center>

```{r}
plot_list$degre_densite
```

</center>

On remarque d'importantes disparités entre les régions en termes de répartition des outliers. Une piste intéressante pourrait être d'identifier des zones géographiques proches et d'estimer un modèle par zone. 

On représente ensuite la distribution des variables quantitatives `surface_relle_bati`, `surface_carrez_lots` et `l_valeur_fonciere` selon que l'observation soit une valeur aberrante ou non. 

```{r}
plot_list <- sapply(
  
  X = c("l_valeur_fonciere", 
        "surface_reelle_bati", 
        "surface_terrain"), 
  
  FUN = distribution_per_category, 
  df = smp_outliers, 
  by = "outlier", 
  multiple = F, 
  
  simplify = F, USE.NAMES = T
  
)
```

<center>

```{r}
plot_list$l_valeur_fonciere
```

</center>

:::: {style="display: flex;"}

::: {}
```{r }
plot_list$surface_reelle_bati
```
:::

::: {}

```{r }
plot_list$surface_terrain
```
:::

::::

Enfin, il apparaît judicieux de déterminer la position géographique des biens identifiés comme valeurs aberrantes de celle des biens "normaux". On remarque une nette dichotomies entre les deux groupes. A première vue, les points aberrants identifiés par la distance de Cook semblent situés dans des régions moins peuplées. Cela est possiblement la conséquence d'avoir mis la région Ile-de-France comme niveau de référence pour la variable `nom_region` dans la régression. 

```{r}
smp_outliers <- smp_outliers %>%
  mutate(
    longitude = dat2 %>%
      filter(rownames(.) %in% c(idxs_smp, idxs_outliers)) %>%
      pull(longitude), 
    latitude = dat2 %>%
      filter(rownames(.) %in% c(idxs_smp, idxs_outliers)) %>%
      pull(latitude)
  )
```


```{r}
factpal <- colorFactor(c("#F9AD9E", "#8CDCDC"), smp_outliers$outlier)

map_outliers <- smp_outliers %>%
  draw_samples() %>%
  leaflet() %>% 
     addTiles() %>% 
     setView(lng = 2.80, lat = 46.80, zoom = 5.1) %>% 
     addCircleMarkers(
       lng = ~longitude,
       lat = ~latitude, 
       radius = 0, 
       color = ~factpal(outlier))
```

<center>

```{r}
map_outliers
```

</center>

Une dichotomie assez nette apparaît entre les biens situés proches des grande villes - qui ne sont pas considérés comme outliers au sens de la distance de Cook - et ceux situés dans des zones + réculées. Un axe d'amélioration serait d'effectuer un modèle de régression par région / par degré de densité. 

#### Modèle sans outlier

On décide d'estimer un troisième modèle en supprimant les outliers identifiés par la méthode de la distance de Cook.

```{r}
dat_mod_no_outliers <- dat_mod %>%
  filter(rownames(.) %in% idxs)
```

```{r}
lm1_no_outliers <- lm(formula = fm, data = dat_mod_no_outliers)
```


```{r eval=FALSE}
stargazer(lm1_no_outliers, type = "text")
```

<center>

```{r results='asis'}
stargazer(lm1_no_outliers, type = "html", title = "Modèle sans outlier") 
```

</center>

<center>

```{r fig.width=5}
N <- 10000
title <- paste0("Logarithme des valeurs foncières après suppression des outliers (N=", N, ")")


df_plot <- data.frame(
  actual = dat_mod_no_outliers$l_valeur_fonciere, 
  fitted = lm1_no_outliers$fitted.values
) %>%
  draw_samples(n_samples = N)
  
regression_plot(df_plot, title) 
```

</center>

La part de la variance expliquée par le modèle passe de 18.2% à 25.6% lorsque l'on supprime les points aberrants. 

#### Modèle sur les outliers

La dernière étape de diagnostic du premier modèle est de relancer l'estimation uniquement sur les biens identifiés comme outliers par la distance de Cook. 

```{r}
dat_mod_outliers <- dat_mod %>%
  filter(rownames(.) %in% idxs_outliers)
```

```{r}
lm1_outliers <- lm(formula = fm, data = dat_mod_outliers)
```


```{r eval=FALSE}
stargazer(lm1_outliers, type = "text")
```

<center>

```{r results='asis'}
stargazer(lm1_outliers, type = "html", title = "Modèle estimé sur les outliers") 
```

</center>

La qualité d'ajustement est bien plus élevée pour le modèle estimé uniquement sur les points aberrants. Plusieurs raisons pourraient expliquer ce résultat :

- le faible nombre d'outliers (`r length(idxs_outliers)`)
- les coefficients estimés sont représentatifs caractéristiques des points aberrants

On peut en déduire que le modèle de régression appliqué à l'ensemble des régions ne semble pas être la meilleure stratégie. 

```{r fig.width=5}
title <- "Logarithme des valeurs foncières sur les points aberrants"

df_plot <- data.frame(
  actual = dat_mod_outliers$l_valeur_fonciere, 
  fitted = lm1_outliers$fitted.values
)
regression_plot(df_plot, title) 
```


### OLS par région 

On se propose d'effectuer une régression par région afin d'estimer les effets des différentes variables explicatives pour chaque région. On pourra par la suite regrouper les régions partageant des dynamiques similaires. 

On travaille sur l'ensemble des régions à l'exception des territoires d'outre-mer (Guadeloupe, Guyane,La Réunion, Martinique).  

Comme précédemment, on utilise la distance de Cook pour identifier les outliers dans les régressions de chaque région et on estime une régression sur le jeu de données diminué de ces outliers ainsi qu'une troisième régression sur les outliers. 

```{r}
x_vars2 <- setdiff(x_vars, "nom_region")

fm2 <- paste(y_var, 
            paste(x_vars2, collapse = "+"), 
            sep = "~") %>%
  as.formula()

regions <- levels(dat_mod$nom_region)
```

```{r eval=FALSE}
pb <- txtProgressBar(min = 0, 
                     max = length(regions), 
                     style = 3,    
                     width = 50,
                     char = "=")

lm_per_region <- list()

for (i in 1:length(regions)) {
  
  region <- regions[i]
  tmp <- dat_mod %>% filter(nom_region == region)
  lm_per_region[[region]] <- lm_process(tmp, fm2) 
  
  setTxtProgressBar(pb, i)
  
}

save(lm_per_region, file = paste0(BACKUP_DIR, "models/lm_per_region.RData"))
```

```{r}
load(paste0(BACKUP_DIR, "models/lm_per_region.RData"))
```

#### Tables de régression  

```{r results='asis'}
for (region in regions) {
  
  results <- lm_per_region[[region]]
  models <- list(results$lm_all, 
                 results$lm_no_outliers,
                 results$lm_outliers)
  
  stargazer(models, 
            type = "html", 
            title = paste("Régression linéaire pour", region), 
            column.labels = c("All", "No outlier", "Outliers"))
}

```
 

#### Métriques par région 

```{r}
metrics <- sapply(
  
  X = c("lm_all", "lm_no_outliers", "lm_outliers"), 
  
  FUN = function(lm_type) { 
    
    tab <- sapply(
  
      X = lm_per_region, 
      
      FUN = function(lm) {
        
        lm_ <- lm[[lm_type]] 
        results <- get_model_rsq(lm_) 
        
        data.frame("rsq" = round(100 * results$rsq, 2), 
                   "adj_rsq" = round(100 * results$rsq_adj, 2))
        
      }, 
    
      simplify = T
      
    ) %>% t() 
    
    colnames(tab) <- c("R² (%)", "Adj. R² (%)") 
    
    return (tab)
    
  }, 
  
  simplify = F, 
  USE.NAMES = T
  
)
```

**Jeu de données complet**

```{r}
datatable(metrics$lm_all)
```

**Sans outlier** 

```{r}
datatable(metrics$lm_no_outliers)
```

**Outliers**

```{r}
datatable(metrics$lm_outliers) 
```






